checkpoint: null #path to lora weights checkpoint BEFORE any wrappers
requires_grad: true #do the prewrapped lora weights require grad? only false for tempscaling

load_mle_checkpoint: false

config:
  _target_: peft.LoraConfig
  task_type: CAUSAL_LM
  inference_mode: false
  init_lora_weights: true #ensures that B weights are init to 0
  r: 8
  #lora_alpha: 16 #we will manually set the alpha to be 2*r when we call instantiate so exclude it here
  lora_dropout: 0.0
  exclude_modules: ".*vision_tower.*" #do not fine-tune vision tower for VLMs
  target_modules: 
    - lm_head
    - q_proj
    - v_proj
