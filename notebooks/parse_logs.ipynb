{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a0e4a-a203-4104-ae5d-2616fd6448df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc6996-6c9b-4cb4-b281-b43fcea635dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(fname):\n",
    "    try:\n",
    "        with open(fname, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34543b4e-8532-44f5-a00a-7fd141ff92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_err(x, y_mean, y_std, label=None, plot_kwargs=None, ax=None):\n",
    "    sort_idx = np.argsort(x)\n",
    "    x_sorted = np.array(x)[sort_idx]\n",
    "    y_mean_sorted = np.array(y_mean)[sort_idx]\n",
    "    \n",
    "\n",
    "    ax.plot(x_sorted, y_mean_sorted, label=label, **plot_kwargs)\n",
    "\n",
    "    if y_std is not None:\n",
    "        y_std_sorted = np.array(y_std)[sort_idx]\n",
    "        y_upper = y_mean_sorted + y_std_sorted\n",
    "        y_lower = y_mean_sorted - y_std_sorted\n",
    "        ax.fill_between(\n",
    "            x_sorted,\n",
    "            y_lower,\n",
    "            y_upper,\n",
    "            alpha=0.1,\n",
    "            color=plot_kwargs['color']\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_with_err_(x, y_mean, y_std, label=None, ax=None, plot_kwargs=None):\n",
    "#def plot_with_err_(x, y_mean, y_std, linestyle=None, label=None, color='blue', marker='.', alpha=0.1, ax=None):\n",
    "    y_upper = y_mean + y_std\n",
    "    y_lower = y_mean - y_std\n",
    "\n",
    "    ax.plot(\n",
    "        x, \n",
    "        y_mean, \n",
    "        label=label, \n",
    "        **plot_kwargs\n",
    "        #linestyle=linestyle, \n",
    "        #color=color, \n",
    "        #marker=marker\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        y_lower,\n",
    "        y_upper,\n",
    "        alpha=alpha,\n",
    "        color=plot_kwargs['color']\n",
    "    )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ed888-8b99-4735-a44b-1c0d39be9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dict = {\n",
    "    \"laplace\": {\n",
    "        \"color\": \"#000000\",\n",
    "        \"linestyle\": (0, (5, 2)),   # dashed\n",
    "        \"marker\": \"o\",\n",
    "        \"linewidth\": 2.4,\n",
    "        \"markersize\": 6.5,\n",
    "        \"markerfacecolor\": \"none\",\n",
    "        \"markeredgewidth\": 1.6,\n",
    "    },\n",
    "    \"mle\": {\n",
    "        \"color\": \"#D55E00\",\n",
    "        \"linestyle\": \"solid\",\n",
    "        \"marker\": \"s\",\n",
    "        \"linewidth\": 2.6,\n",
    "        \"markersize\": 6.2,\n",
    "        \"markerfacecolor\": \"#D55E00\",\n",
    "        \"markeredgecolor\": \"white\",\n",
    "        \"markeredgewidth\": 0.8,\n",
    "    },\n",
    "    \"map\": {\n",
    "        \"color\": \"#7F7F7F\",\n",
    "        \"linestyle\": (0, (1, 1)),   # dotted\n",
    "        \"marker\": \"D\",\n",
    "        \"linewidth\": 2.2,\n",
    "        \"markersize\": 6.0,\n",
    "        \"markerfacecolor\": \"none\",\n",
    "        \"markeredgewidth\": 1.4,\n",
    "    },\n",
    "    \"tempscale\": {\n",
    "        \"color\": \"#0072B2\",\n",
    "        \"linestyle\": \"dashdot\",\n",
    "        \"marker\": \"^\",\n",
    "        \"linewidth\": 2.4,\n",
    "        \"markersize\": 6.8,\n",
    "        \"markerfacecolor\": \"#0072B2\",\n",
    "        \"markeredgecolor\": \"white\",\n",
    "        \"markeredgewidth\": 0.8,\n",
    "    },\n",
    "    \"blob\": {\n",
    "        \"color\": \"#CC79A7\",\n",
    "        \"linestyle\": (0, (3, 1, 1, 1)),  # short dash-dot pattern\n",
    "        \"marker\": \"P\",                   # plus-filled\n",
    "        \"linewidth\": 2.2,\n",
    "        \"markersize\": 7.0,\n",
    "        \"markerfacecolor\": \"#CC79A7\",\n",
    "        \"markeredgecolor\": \"white\",\n",
    "        \"markeredgewidth\": 0.8,\n",
    "    },\n",
    "    \"scalabl\": {\n",
    "        \"color\": \"#009E73\",\n",
    "        \"linestyle\": (0, (7, 2)),   # long dashed\n",
    "        \"marker\": \"v\",\n",
    "        \"linewidth\": 2.6,\n",
    "        \"markersize\": 6.8,\n",
    "        \"markerfacecolor\": \"none\",\n",
    "        \"markeredgewidth\": 1.6,\n",
    "    },\n",
    "    \"tfb\": {\n",
    "        \"color\": \"#56B4E9\",\n",
    "        \"linestyle\": (0, (2, 2)),   # evenly dashed\n",
    "        \"marker\": \"X\",\n",
    "        \"linewidth\": 2.3,\n",
    "        \"markersize\": 6.8,\n",
    "        \"markerfacecolor\": \"#56B4E9\",\n",
    "        \"markeredgecolor\": \"black\",\n",
    "        \"markeredgewidth\": 0.6,\n",
    "    },\n",
    "    \"mcdropout\": {\n",
    "        \"color\": \"#E69F00\",\n",
    "        \"linestyle\": (0, (1, 2)),   # spaced dots\n",
    "        \"marker\": \"<\",\n",
    "        \"linewidth\": 2.4,\n",
    "        \"markersize\": 6.8,\n",
    "        \"markerfacecolor\": \"#E69F00\",\n",
    "        \"markeredgecolor\": \"white\",\n",
    "        \"markeredgewidth\": 0.8,\n",
    "    },\n",
    "    \"deepensemble\": {\n",
    "        \"color\": \"#332288\",\n",
    "        \"linestyle\": (0, (4, 1, 1, 1, 1, 1)),  # dash-dot-dot\n",
    "        \"marker\": \">\",\n",
    "        \"linewidth\": 2.4,\n",
    "        \"markersize\": 6.8,\n",
    "        \"markerfacecolor\": \"none\",\n",
    "        \"markeredgewidth\": 1.6,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793a5a5-2916-450e-88ec-888e3613b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dict = {\n",
    "    'laplace': {'color': 'black', 'linestyle': '--', 'marker': '.'},\n",
    "    'mle': {'color': 'red', 'linestyle': ':', 'marker': 'v'},\n",
    "    'map': {'color': 'grey', 'linestyle': ':', 'marker': 'v'},\n",
    "    'tempscale': {'color': 'blue', 'linestyle': 'dashdot', 'marker': 'o'},\n",
    "    'blob': {'color': 'purple', 'linestyle': '--', 'marker': 's'},\n",
    "    'scalabl': {'color': 'green', 'linestyle': 'solid', 'marker': '^'},\n",
    "    'tfb': {'color': 'blue', 'linestyle': 'dashdot', 'marker': '^'},\n",
    "    'mcdropout': {'color': 'orange', 'linestyle': 'dashdot', 'marker': 'v'},\n",
    "    'deepensemble': {'color': 'teal', 'linestyle': 'dashdot', 'marker': 'v'},\n",
    "    #deepens\n",
    "    #mcdroput\n",
    "    #sgld?\n",
    "    #map\n",
    "    #zeroshot?\n",
    "}\n",
    "metric2arrow = {\n",
    "    'ACC': '↑',\n",
    "    'ECE': '↓',\n",
    "    'NLL': '↓',\n",
    "    'Brier': '↓',\n",
    "    'peak_memory': '↓',\n",
    "    'latency': '↓',\n",
    "}\n",
    "\n",
    "wrapper2label = {\n",
    "    'mle': 'MLE',\n",
    "    'blob': 'BLoB',\n",
    "    'scalabl': 'ScalaBL',\n",
    "    'laplace': 'Laplace',\n",
    "    'tfb': 'TFB',\n",
    "    'mcdropout': 'MCDropout',\n",
    "    'tempscale': 'TempScale',\n",
    "    'deepensemble': 'ENS',\n",
    "    'map': 'MAP',\n",
    "    'tempscale': 'TempScale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39256e35-e6a1-49c2-bcf1-3e82feaaffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = '/workspace1/csamplawski/src/BayesAdapt/logs/'\n",
    "root = '/project/synthesis/bayesadapt/logs/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d15a7-bb68-4854-b35c-52effec0dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "/project/synthesis/bayesadapt/logs/Qwen/Qwen3-VL-4B-Instruct/16bit/mle/rank8/vlm/seed0/srqa/results/active_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237c22d-b80d-49c9-8e57-065314b163fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_fnames = glob.glob(f'{root}/**/active_learn/results.json', recursive=True)\n",
    "expdirs = []\n",
    "for fname in json_fnames:\n",
    "    tokens = fname.split('/')\n",
    "    edir = '/'.join(tokens[0:-1])\n",
    "    expdirs.append(edir)\n",
    "expdirs = list(set(expdirs))\n",
    "\n",
    "df = []\n",
    "for edir in expdirs:\n",
    "    tokens = edir.replace(root, '').split('/')\n",
    "    keys = ['model', 'quant', 'wrapper', 'rank', 'prompt_type', 'seed', 'dataset']\n",
    "    row = dict(zip(keys, tokens[1:]))\n",
    "    row['rank'] = int(tokens[4].replace('rank', ''))\n",
    "    row['seed'] = int(tokens[6][-1])\n",
    "    data = load_json(f'{edir}/results.json')\n",
    "    for metric in ['ACC', 'ECE', 'NLL', 'Brier']:\n",
    "        row[metric] = [item['test_metrics'][0][metric] for item in data]\n",
    "    #row['results'] = data\n",
    "    df.append(row)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "def mean_std_vectors(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    series: metric column within one group (one value per seed),\n",
    "            where each value is a vector (list/ndarray) of y-values.\n",
    "    returns: (mean_vector, std_vector)\n",
    "    \"\"\"\n",
    "    arrs = [np.asarray(v, dtype=float) for v in series]\n",
    "    lengths = {a.shape for a in arrs}\n",
    "    if len(lengths) != 1:\n",
    "        raise ValueError(f\"Vector shapes differ within group: {lengths}\")\n",
    "    stacked = np.stack(arrs, axis=0)          # (n_seeds, T)\n",
    "    return pd.Series({\n",
    "        \"mean\": stacked.mean(axis=0),\n",
    "        \"std\":  stacked.std(axis=0, ddof=1) if stacked.shape[0] > 1 else np.zeros(stacked.shape[1]),\n",
    "    })\n",
    "\n",
    "group_cols = ['model', 'quant', 'wrapper', 'rank', 'prompt_type', 'dataset']\n",
    "\n",
    "agg_parts = []\n",
    "for m in metrics:\n",
    "    tmp = (\n",
    "        df.groupby(group_cols)[m]\n",
    "          .apply(mean_std_vectors)\n",
    "          .unstack()  # columns: mean/std\n",
    "    )\n",
    "    # rename to ACC_mean / ACC_std, etc.\n",
    "    tmp.columns = [f\"{m}_{c}\" for c in tmp.columns]\n",
    "    agg_parts.append(tmp)\n",
    "\n",
    "active_df = pd.concat(agg_parts, axis=1).reset_index()\n",
    "active_df = active_df.set_index(group_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f559cbd-b462-4d93-8fb1-8206304fb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b6548-d5ec-4651-bcae-10c4bfccc273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b6c80-9401-4d39-bb1e-af4d7d23522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = plt.gca()\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 5), sharey=False)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "metrics = ['ACC', 'ECE', 'NLL', 'Brier']\n",
    "\n",
    "dataset = 'srqa'\n",
    "prompt_type = 'vlm'\n",
    "quant = '16bit'\n",
    "rank = 8\n",
    "\n",
    "#base_query_str = f\"dataset == '{dataset}' and prompt_type == '{prompt_type}' and quant == '{quant}' and rank == {rank}\"\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    arrow = metric2arrow[metric]\n",
    "    for wrapper in ['mle', 'scalabl', 'mcdropout','map','blob']:\n",
    "        label = wrapper2label[wrapper]\n",
    "        qdf = query(active_df, prompt_type=prompt_type, wrapper=wrapper, dataset=dataset)\n",
    "        y_mean = qdf[f'{metric}_mean'][0]\n",
    "        y_std = qdf[f'{metric}_std'][0]\n",
    "        y_std=None\n",
    "        x = np.arange(len(y_mean)) + 1\n",
    "        x *= 10\n",
    "        ax = plot_with_err(x, y_mean, y_std, plot_kwargs=style_dict[wrapper], label=label, ax=ax)\n",
    "\n",
    "\n",
    "    ax.set_xlabel('# Labels Acquired')\n",
    "    ax.set_ylabel(f\"{metric} ({arrow})\")\n",
    "    #ax.legend(\n",
    "    #    loc='upper center',          # Anchor point on the legend box itself\n",
    "    #    bbox_to_anchor=(0.5, -0.15), # (x, y) coordinates relative to the plot axes\n",
    "    #    ncols=2,       # Forces all items into a single row\n",
    "    #    frameon=True                # Optional: removes the box border for a cleaner look\n",
    "    #)\n",
    "    #ax.set_title(f'Qwen3 Family | {prompt_type} | rank = {rank} | {dataset}')\n",
    "    ax.grid()\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles, labels,\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncols=9,          # adjust for readability\n",
    "    frameon=True\n",
    ")\n",
    "fig.suptitle(f'Qwen3-8B-VL-Instruct Active Learning on SymbolicRegressionQA',y=0.95)\n",
    "fig.subplots_adjust(bottom=0.15)\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6271e4a-82b2-4a4c-adf7-b4da385f689e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ebfb-09a5-45f9-a4ec-37b2f116249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_keys = ['model', 'quant', 'wrapper', 'rank', 'prompt_type', 'dataset', 'num_base', 'num_trainable_params', 'num_total_params']\n",
    "metric_keys = ['ACC', 'ECE', 'NLL', 'Brier', 'peak_memory', 'latency']\n",
    "\n",
    "json_fnames = glob.glob(f'{root}/**/id/metrics.json', recursive=True)\n",
    "\n",
    "expdirs = []\n",
    "for fname in json_fnames:\n",
    "    tokens = fname.split('/')\n",
    "    edir = '/'.join(tokens[0:-1])\n",
    "    expdirs.append(edir)\n",
    "expdirs = list(set(expdirs))\n",
    "\n",
    "df = []\n",
    "for edir in expdirs:\n",
    "    tokens = edir.replace(root, '').split('/')\n",
    "    keys = ['model', 'quant', 'wrapper', 'rank', 'prompt_type', 'seed', 'dataset']\n",
    "    row = dict(zip(keys, tokens[1:]))\n",
    "    row['rank'] = int(tokens[4].replace('rank', ''))\n",
    "    row['seed'] = int(tokens[6][-1])\n",
    "    data = load_json(f'{edir}/metrics.json')\n",
    "    row['results'] = data\n",
    "    df.append(row)\n",
    "df = pd.DataFrame(df)\n",
    "df_exploded = df.explode('results').reset_index(drop=True)\n",
    "metrics_df = pd.json_normalize(df_exploded['results']).drop(columns=['seed'])\n",
    "id_df_seeds = pd.concat([df_exploded.drop(columns=['results']), metrics_df], axis=1)\n",
    "id_df = id_df_seeds.groupby(exp_keys)[metric_keys].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8071be5-2b58-4deb-b077-a11c9b65548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222df348-02bf-4e90-84fd-630720ac1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(id_df, dataset='expression_logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ac944-1e5d-4000-a51c-f1f6f03de2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(ood_df, dataset='expression_logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a62be9-d9b6-43c4-94c3-63df79b59123",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(id_df, dataset='circuit_logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb754b02-913c-46cd-8d34-7408146feec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(ood_df, dataset='circuit_logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd1ac1-f58b-432d-bdec-4609eb1fa910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "METHODS_MAP = {\n",
    "    \"MLE\": \"mle\",\n",
    "    \"MAP\": \"map\",\n",
    "    \"MC-Dropout\": \"mcdropout\",\n",
    "    \"Ensemble\": \"deepensemble\",\n",
    "    \"Laplace\": \"laplace\",\n",
    "    \"BLoB\": \"blob\",\n",
    "    \"ScalaBL\": \"scalabl\",\n",
    "    \"TFB\": \"tfb\"\n",
    "}\n",
    "\n",
    "def latex_escape(s: str) -> str:\n",
    "    # at minimum underscore; plus a few common LaTeX specials\n",
    "    repl = {\n",
    "        \"\\\\\": r\"\\textbackslash{}\",\n",
    "        \"&\": r\"\\&\",\n",
    "        \"%\": r\"\\%\",\n",
    "        \"$\": r\"\\$\",\n",
    "        \"#\": r\"\\#\",\n",
    "        \"_\": r\"\\_\",\n",
    "        \"{\": r\"\\{\",\n",
    "        \"}\": r\"\\}\",\n",
    "        \"~\": r\"\\textasciitilde{}\",\n",
    "        \"^\": r\"\\textasciicircum{}\",\n",
    "    }\n",
    "    out = []\n",
    "    for ch in str(s):\n",
    "        out.append(repl.get(ch, ch))\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        flat = []\n",
    "        for a, b in df.columns.to_flat_index():\n",
    "            if b is None or b == \"\":\n",
    "                flat.append(str(a))\n",
    "            else:\n",
    "                flat.append(f\"{a}_{b}\")\n",
    "        df.columns = flat\n",
    "    return df\n",
    "\n",
    "def _prep_df(id_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # make exp_keys queryable as columns, and make metric columns like ACC_mean/ACC_std\n",
    "    df = id_df.reset_index() if isinstance(id_df.index, pd.MultiIndex) else id_df.copy()\n",
    "    return _flatten_columns(df)\n",
    "\n",
    "def _format_pm(mean: float, std: float, metric: str) -> str:\n",
    "    # If ACC/ECE look like fractions, auto-convert to percent\n",
    "    if metric in (\"ACC\", \"ECE\") and mean <= 1.0:\n",
    "        mean *= 100.0\n",
    "        std *= 100.0\n",
    "\n",
    "    # per-metric formatting (tweak if you want)\n",
    "    if metric in (\"ACC\", \"ECE\"):\n",
    "        return f\"${mean:.2f}_{{\\\\pm {std:.1f}}}$\"\n",
    "    else:\n",
    "        return f\"${mean:.3f}_{{\\\\pm {std:.3f}}}$\"\n",
    "\n",
    "def make_latex_table(\n",
    "    id_df: pd.DataFrame,\n",
    "    model: str,\n",
    "    rank: int,\n",
    "    datasets: list[str],\n",
    "    *,\n",
    "    prompt_type: str = \"instruct\",\n",
    "    quant: str = \"16bit\",\n",
    "    metrics: list[str] = (\"ACC\", \"ECE\", \"NLL\"),\n",
    "    methods_map: dict[str, str] = METHODS_MAP,\n",
    "    caption: str | None = None,\n",
    ") -> str:\n",
    "    df = _prep_df(id_df)\n",
    "\n",
    "    # sanity: required columns\n",
    "    required = {\"model\", \"rank\", \"prompt_type\", \"quant\", \"dataset\", \"wrapper\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"id_df is missing required columns after prep: {sorted(missing)}\")\n",
    "\n",
    "    ncols = 2 + len(datasets)\n",
    "    col_spec = \"@{}\" + (\"c\" * ncols) + \"@{}\"\n",
    "\n",
    "    ds_headers = [latex_escape(d) for d in datasets]\n",
    "    header = (\n",
    "        \"\\\\textbf{Metric} & \\\\textbf{Method} & \"\n",
    "        + \" & \".join([f\"\\\\textbf{{{h}}}\" for h in ds_headers])\n",
    "        + \" \\\\\\\\\\n\"\n",
    "    )\n",
    "\n",
    "    if caption is None:\n",
    "        caption = f\"Performance Comparison ({latex_escape(model)}, rank {rank})\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"\\\\begin{table*}[h!]\")\n",
    "    lines.append(\"\\\\centering\")\n",
    "    lines.append(f\"\\\\caption{{{caption}}}\")\n",
    "    lines.append(f\"\\\\begin{{tabular}}{{{col_spec}}}\")\n",
    "    lines.append(\"\\\\toprule\")\n",
    "    lines.append(header.rstrip(\"\\n\"))\n",
    "    lines.append(\"\\\\midrule\")\n",
    "\n",
    "    up_metrics = {\"ACC\"}  # everything else treated as \"down\"\n",
    "    method_display_names = list(methods_map.keys())\n",
    "\n",
    "    for mi, metric in enumerate(metrics):\n",
    "        arrow = \"\\\\uparrow\" if metric in up_metrics else \"\\\\downarrow\"\n",
    "        lines.append(f\"\\\\multirow{{{len(method_display_names)}}}{{*}}{{\\\\textbf{{{latex_escape(metric)} ($${arrow}$$)}}}}\".replace(\"$$\", \"$\"))\n",
    "\n",
    "        for display_name, wrapper_code in methods_map.items():\n",
    "            row = [f\"& {latex_escape(display_name)}\"]\n",
    "\n",
    "            for ds in datasets:\n",
    "                sub = df[\n",
    "                    (df[\"model\"] == model)\n",
    "                    & (df[\"rank\"] == rank)\n",
    "                    & (df[\"prompt_type\"] == prompt_type)\n",
    "                    & (df[\"quant\"] == quant)\n",
    "                    & (df[\"dataset\"] == ds)\n",
    "                    & (df[\"wrapper\"] == wrapper_code)\n",
    "                ]\n",
    "\n",
    "                if sub.empty:\n",
    "                    row.append(\"& TBD\")\n",
    "                    continue\n",
    "\n",
    "                # if multiple rows match (e.g., because num_* params differ), pick a stable choice\n",
    "                if len(sub) > 1:\n",
    "                    pick_col = \"num_trainable_params\" if \"num_trainable_params\" in sub.columns else None\n",
    "                    if pick_col:\n",
    "                        sub = sub.sort_values(pick_col, ascending=False)\n",
    "                    sub = sub.iloc[[0]]\n",
    "\n",
    "                mean_col = f\"{metric}_mean\"\n",
    "                std_col = f\"{metric}_std\"\n",
    "                if mean_col not in sub.columns or std_col not in sub.columns:\n",
    "                    row.append(\"& TBD\")\n",
    "                    continue\n",
    "\n",
    "                mean = float(sub[mean_col].iloc[0])\n",
    "                std = float(sub[std_col].iloc[0])\n",
    "                row.append(f\"& {_format_pm(mean, std, metric)}\")\n",
    "\n",
    "            lines.append(\" \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "        if mi != len(metrics) - 1:\n",
    "            lines.append(\"\\\\midrule\")\n",
    "\n",
    "    lines.append(\"\\\\bottomrule\")\n",
    "    lines.append(\"\\\\end{tabular}\")\n",
    "    lines.append(\"\\\\end{table*}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Example ---\n",
    "# latex = make_latex_table(\n",
    "#     id_df,\n",
    "#     model=\"Qwen3-8B\",\n",
    "#     rank=8,\n",
    "#     datasets=[\"winogrande_s\", \"ARC-Challenge\", \"ARC-Easy\", \"obqa\", \"boolq\"],\n",
    "#     prompt_type=\"instruct\",\n",
    "#     quant=\"16bit\",\n",
    "#     metrics=[\"ACC\", \"ECE\", \"NLL\"],\n",
    "# )\n",
    "# print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92314848-a4e7-42f6-82e4-d11933dc9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = make_latex_table(\n",
    "    id_df,\n",
    "    model=\"Qwen3-8B\",\n",
    "    rank=8,\n",
    "    datasets=[\"winogrande_xs\", \"winogrande_s\", \"winogrande_m\", \"winogrande_l\"],\n",
    "    prompt_type=\"instruct\",\n",
    "    quant=\"16bit\",\n",
    "    metrics=[\"ACC\", \"ECE\", \"NLL\", \"Brier\"],\n",
    ")\n",
    "latex = make_latex_table(\n",
    "    id_df,\n",
    "    model=\"Qwen3-VL-8B-Instruct\",\n",
    "    rank=8,\n",
    "    datasets=[\"slake\", \"mmstar\", \"MathVerse\"],\n",
    "    prompt_type=\"vlm\",\n",
    "    quant=\"16bit\",\n",
    "    metrics=[\"ACC\", \"ECE\", \"NLL\", \"Brier\"],\n",
    ")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22c6ae-97b1-4be4-8b40-7ddffba07eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12c02c-1456-47b0-ada3-14f346dd1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(df, model=None, dataset=None, wrapper=None, prompt_type='instruct', quant='16bit', rank=8):\n",
    "    query_str = f\"prompt_type == '{prompt_type}' and quant == '{quant}' and rank == {rank}\"\n",
    "    if model is not None:\n",
    "        query_str += f\" and model == '{model}'\"\n",
    "    if dataset is not None:\n",
    "        query_str += f\" and dataset == '{dataset}'\"\n",
    "    if wrapper is not None:\n",
    "        query_str += f\" and wrapper == '{wrapper}'\"\n",
    "    q = df.query(query_str).reset_index()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa088da7-5d5f-46a5-bc0c-b11f6dc34891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8e5c2-34c5-4c77-bb51-491fab095023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- example ----\n",
    "# tables = generate_multidataset_metric_tables(\n",
    "#     id_df,\n",
    "#     model=\"Qwen3-8B\",\n",
    "#     rank=8,\n",
    "#     datasets=[\"winogrande_s\",\"ARC-Challenge\",\"ARC-Easy\",\"winogrande_m\",\"obqa\",\"boolq\"],\n",
    "#     prompt_type=\"instruct\",\n",
    "#     quant=\"16bit\",\n",
    "#     scale_acc_ece_to_percent=False,  # flip if needed\n",
    "#     caption=\"Performance Comparison (Qwen3-8B, rank=8)\",\n",
    "#     label_prefix=\"tab:qwen3_8b_r8\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c6863-0ff9-47d8-8590-1e1b3054570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric, wrapper_key, ds,\n",
    "      \"sub_rows=\", len(sub),\n",
    "      \"picked_std=\", None if len(sub)==0 else float(get_stat(sub.iloc[0], metric, \"std\")),\n",
    "      \"all_stds=\", [] if len(sub)==0 else [float(x) for x in sub[(metric,\"std\")].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae23b3-a0bf-453d-880c-58a658ad848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(id_df, dataset='winogrande_s', wrapper='mle', model='Qwen3-8B', rank=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd5bd6-9b60-40cb-bf21-32fbe2ac462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(id_df_seeds, prompt_type='vlm', wrapper='deepensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d566607-65ab-4dd0-88af-0bbc111d8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_fnames = glob.glob(f'{root}/**/ood/**/metrics.json', recursive=True)\n",
    "expdirs = []\n",
    "for fname in json_fnames:\n",
    "    tokens = fname.split('/')\n",
    "    edir = '/'.join(tokens[0:-1])\n",
    "    expdirs.append(edir)\n",
    "expdirs = list(set(expdirs))\n",
    "\n",
    "df = []\n",
    "for edir in expdirs:\n",
    "    tokens = edir.replace(root, '').split('/')\n",
    "    keys = ['model', 'quant', 'wrapper', 'rank', 'prompt_type', 'seed']\n",
    "    row = dict(zip(keys, tokens[1:-2]))\n",
    "    row['rank'] = int(tokens[4].replace('rank', ''))\n",
    "    row['seed'] = int(tokens[6][-1])\n",
    "    row['dataset'] = tokens[-1]\n",
    "    data = load_json(f'{edir}/metrics.json')\n",
    "    row['results'] = data\n",
    "    df.append(row)\n",
    "df = pd.DataFrame(df)\n",
    "df_exploded = df.explode('results').reset_index(drop=True)\n",
    "metrics_df = pd.json_normalize(df_exploded['results']).drop(columns=['seed'])\n",
    "ood_df_seeds = pd.concat([df_exploded.drop(columns=['results']), metrics_df], axis=1)\n",
    "ood_df = ood_df_seeds.groupby(exp_keys)[metric_keys].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946ebbd-dd7c-4669-866c-97b4d4c0fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(25, 5), sharey=False)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "metrics = ['ACC', 'ECE', 'NLL', 'Brier']\n",
    "\n",
    "noise_stds = [0,1,2,4,8,16,32,64,128]\n",
    "x = np.arange(len(noise_stds))\n",
    "\n",
    "dataset = 'slake'\n",
    "prompt_type = 'vlm'\n",
    "quant = '16bit'\n",
    "rank = 8\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    arrow = metric2arrow[metric]\n",
    "    \n",
    "    for wrapper in ['mle', 'blob', 'scalabl']:\n",
    "        label = wrapper2label[wrapper]\n",
    "        y_mean, y_std = [], []\n",
    "        for std in noise_stds:\n",
    "            if std == 0:\n",
    "                dataset = 'slake'\n",
    "                metric_df = id_df\n",
    "            else:\n",
    "                dataset = f'noisy_slake{std}'\n",
    "                metric_df = ood_df\n",
    "            \n",
    "            metric_vals = metric_df.query(f\"dataset == '{dataset}' and prompt_type == '{prompt_type}' and wrapper == '{wrapper}' and quant == '{quant}' and rank == {rank} and model == 'Qwen3-VL-8B-Instruct'\" ).reset_index()[metric]\n",
    "            y_mean.append(metric_vals['mean'].item())\n",
    "            y_std.append(metric_vals['std'].item())\n",
    "        ax = plot_with_err(x, y_mean, y_std, **style_dict[wrapper], label=label, ax=ax)\n",
    "        ax.set_xlabel('Noise STD (pixel units)')\n",
    "        \n",
    "    ax.set_ylabel(f\"{metric} ({arrow})\")\n",
    "    ax.legend(\n",
    "        loc='upper center',          # Anchor point on the legend box itself\n",
    "        bbox_to_anchor=(0.5, -0.15), # (x, y) coordinates relative to the plot axes\n",
    "        ncols=2,       # Forces all items into a single row\n",
    "        frameon=True                # Optional: removes the box border for a cleaner look\n",
    "    )\n",
    "    \n",
    "    #ax.set_xscale('log', base=2)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(noise_stds)\n",
    "    ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be2b0a-91e7-45a6-8348-e343b307cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(ood_df_seeds, wrapper='scalabl', prompt_type='vlm', dataset='noisy_slake8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191bd34f-3a29-4a8b-8fa2-743eec54bc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16a0cf-f8d3-4932-99f0-eb03a2951130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = plt.gca()\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 5), sharey=False)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "metrics = ['ACC', 'ECE', 'NLL', 'Brier']\n",
    "\n",
    "dataset = 'obqa'\n",
    "prompt_type = 'instruct'\n",
    "quant = '16bit'\n",
    "rank = 8\n",
    "\n",
    "#base_query_str = f\"dataset == '{dataset}' and prompt_type == '{prompt_type}' and quant == '{quant}' and rank == {rank}\"\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    arrow = metric2arrow[metric]\n",
    "    for wrapper in ['mle', 'scalabl', 'blob', 'mcdropout','tfb','laplace','deepensemble', 'map', 'tempscale']:\n",
    "    #for wrapper in ['mle', 'laplace', 'tfb' ,'deepensemble']:\n",
    "        label = wrapper2label[wrapper]\n",
    "        #metric_df = id_df.groupby(exp_keys)[metric].agg(['mean', 'std'])\n",
    "        #query_str = base_query_str + f\" and wrapper == '{wrapper}'\"\n",
    "        #q = id_df.query(query_str).reset_index()\n",
    "        q = query(id_df, prompt_type=prompt_type, wrapper=wrapper, dataset=dataset)\n",
    "        ax = plot_with_err(q['num_base'] / 10**9, q[(metric, 'mean')], None, plot_kwargs=style_dict[wrapper], label=label, ax=ax)\n",
    "        #ax = plot_with_err(q['num_base'], q['num_trainable_params'], None, **style_dict[wrapper], label=label, ax=ax)\n",
    "\n",
    "\n",
    "    ax.set_xlabel('# Base Parameters (billions)')\n",
    "    ax.set_ylabel(f\"{metric} ({arrow})\")\n",
    "    #ax.legend(\n",
    "    #    loc='upper center',          # Anchor point on the legend box itself\n",
    "    #    bbox_to_anchor=(0.5, -0.15), # (x, y) coordinates relative to the plot axes\n",
    "    #    ncols=2,       # Forces all items into a single row\n",
    "    #    frameon=True                # Optional: removes the box border for a cleaner look\n",
    "    #)\n",
    "    #ax.set_title(f'Qwen3 Family | {prompt_type} | rank = {rank} | {dataset}')\n",
    "    ax.grid()\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles, labels,\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncols=9,          # adjust for readability\n",
    "    frameon=True\n",
    ")\n",
    "fig.suptitle(f'Qwen3 Family on In-Distribution {dataset}',y=0.95)\n",
    "fig.subplots_adjust(bottom=0.15)\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02473e-eae7-46a7-80c1-d55aa53a0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "qdf = query(id_df_seeds, dataset='mmstar', prompt_type='vlm', wrapper='tfb')\n",
    "print(qdf.shape)\n",
    "qdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a17070-86f1-44a7-af36-24ffd3d48a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = plt.gca()\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 5), sharey=False)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "metrics = ['ACC', 'ECE', 'NLL', 'Brier']\n",
    "\n",
    "dataset_sizes = ['xs','s','m','l']\n",
    "x_vals = [160,640,2558,10234]\n",
    "prompt_type = 'instruct'\n",
    "quant = '16bit'\n",
    "model = 'Qwen3-8B'\n",
    "rank = 8\n",
    "\n",
    "base_query_str = f\"model == '{model}' and prompt_type == '{prompt_type}' and quant == '{quant}' and rank == {rank}\"\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    arrow = metric2arrow[metric]\n",
    "    for wrapper in ['mle', 'scalabl', 'blob', 'mcdropout', 'laplace','tfb', 'deepensemble', 'map', 'tempscale']:\n",
    "        label = wrapper2label[wrapper]\n",
    "        y_mean, y_std = [], []\n",
    "        for size in dataset_sizes:\n",
    "            query_str = base_query_str + f\" and wrapper == '{wrapper}' and dataset == 'winogrande_{size}'\"\n",
    "            #metric_df = id_df.groupby(exp_keys)[metric].agg(['mean', 'std'])\n",
    "            q = id_df.query(query_str).reset_index()\n",
    "            try:\n",
    "                y_mean.append(q[(metric, 'mean')].item())\n",
    "                y_std.append(q[(metric, 'std')].item())\n",
    "            except:\n",
    "                continue\n",
    "        ax = plot_with_err(x_vals[0:len(y_mean)], y_mean, None, plot_kwargs=style_dict[wrapper], label=label, ax=ax)\n",
    "    ax.grid()\n",
    "    ax.set_ylabel(f\"{metric} ({arrow})\")\n",
    "    ax.set_xlabel('Training Set Size (# of Instances)')\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles, labels,\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncols=9,          # adjust for readability\n",
    "    frameon=True\n",
    ")\n",
    "fig.suptitle(f'Qwen3-8B In-Distribution Winogrande',y=0.95)\n",
    "fig.subplots_adjust(bottom=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8e6d7-dc4a-4620-9575-0f8b3e4ae7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
